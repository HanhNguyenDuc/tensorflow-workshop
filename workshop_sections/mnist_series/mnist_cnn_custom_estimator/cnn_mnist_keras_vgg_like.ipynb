{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "  http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some imports and define some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Convolutional Neural Network Custom Estimator for MNIST, built with tf.layers.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "# To use 'fashion mnist', instead point DATA_DIR to a directory containing\n",
    "# these files: https://github.com/zalandoresearch/fashion-mnist#get-the-data\n",
    "DATA_DIR = '/tmp/MNIST_data'\n",
    "MODEL_DIR = os.path.join(\"/tmp/tfmodels/mnist_cnn_estimator\",\n",
    "                          \"keras_\" + str(int(time.time())))\n",
    "# This is too short for proper training (especially with 'Fashion-MNIST'), \n",
    "# but we'll use it here to make the notebook quicker to run.\n",
    "NUM_STEPS = 1000\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(\"using model dir: %s\" % MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model function that will be used in constructing the Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "  \"\"\"Model function for CNN.\"\"\"\n",
    "\n",
    "  # Input Layer\n",
    "  # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "  # MNIST images are 28x28 pixels, and have one color channel\n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    K.set_learning_phase(1)\n",
    "  else:\n",
    "    K.set_learning_phase(0)\n",
    "\n",
    "  conv1 = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\",\n",
    "            input_shape=(28,28,1), activation='relu')(input_layer)\n",
    "  conv2 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", activation='relu')(conv1)\n",
    "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "  dropout1 = Dropout(0.5)(pool1)\n",
    "  conv3 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation='relu')(dropout1)\n",
    "  conv4 = Conv2D(filters=256, kernel_size=(3, 3), padding=\"valid\", activation='relu')(conv3)\n",
    "  pool2 = MaxPooling2D(pool_size=(3, 3))(conv4)\n",
    "  dropout2 = Dropout(0.5)(pool2)\n",
    "  pool2_flat = Flatten()(dropout2)\n",
    "  dense1 = Dense(256)(pool2_flat)\n",
    "  lrelu = LeakyReLU()(dense1)\n",
    "  dropout3 = Dropout(0.5)(lrelu)\n",
    "  dense2 = Dense(256)(dropout3)\n",
    "  lrelu2 = LeakyReLU()(dense2)\n",
    "  logits = Dense(10, activation='linear')(lrelu2)\n",
    "\n",
    "  predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "  prediction_output = tf.estimator.export.PredictOutput({\"classes\": tf.argmax(input=logits, axis=1),\n",
    "     \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")})\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions,\n",
    "        export_outputs={tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: prediction_output})\n",
    "\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "  loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "  # Generate some summary info\n",
    "  tf.summary.scalar('loss', loss)\n",
    "  tf.summary.histogram('conv1', conv1)\n",
    "  tf.summary.histogram('dense', dense)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an input function for reading in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_input_fn(dataset, batch_size=BATCH_SIZE):\n",
    "    def _input_fn():\n",
    "        X = tf.constant(dataset.images)\n",
    "        Y = tf.constant(dataset.labels, dtype=tf.int32)\n",
    "        image_batch, label_batch = tf.train.shuffle_batch([X,Y],\n",
    "                               batch_size=batch_size,\n",
    "                               capacity=8*batch_size,\n",
    "                               min_after_dequeue=4*batch_size,\n",
    "                               enqueue_many=True\n",
    "                              )\n",
    "        return {'x': image_batch} , label_batch\n",
    "\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training and eval data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training and eval data\n",
    "\n",
    "mnist = input_data.read_data_sets(DATA_DIR)\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "\n",
    "eval_data = mnist.test.images  # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "predict_data_batch = mnist.test.next_batch(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Estimator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the Estimator\n",
    "mnist_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=MODEL_DIR)\n",
    "\n",
    "# Set up logging for predictions\n",
    "# Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model.  Pass the estimator object the input function to use, and some other config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "mnist_classifier.train(\n",
    "    input_fn=generate_input_fn(mnist.train, batch_size=BATCH_SIZE),\n",
    "    steps=NUM_STEPS,\n",
    "    hooks=[logging_hook]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model and print results\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": eval_data},\n",
    "    y=eval_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a few predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": predict_data_batch[0]},\n",
    "    y=np.asarray(predict_data_batch[1], dtype=np.int32),\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "predict_results = mnist_classifier.predict(input_fn=predict_input_fn)\n",
    "for i, p in enumerate(predict_results):\n",
    "    print(\"Correct label: %s\" % predict_data_batch[1][i])\n",
    "    print(\"Prediction: %s\" % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, export the model for later serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    feature_tensor = tf.placeholder(tf.float32, [None, 784])\n",
    "    return tf.estimator.export.ServingInputReceiver({'x': feature_tensor}, {'x': feature_tensor})\n",
    "exported_model_dir = mnist_classifier.export_savedmodel(MODEL_DIR, serving_input_receiver_fn)\n",
    "decoded_model_dir = exported_model_dir.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the characteristics of the exported model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir $decoded_model_dir --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at info about our training run. Start up TensorBoard as follows in a new terminal window, pointing it to the MODEL_DIR. (If you get a 'not found' error, make sure you've activated your virtual environment in that new window):\n",
    "```\n",
    "$ tensorboard --logdir=<model_dir>\n",
    "```\n",
    "\n",
    "Try the following to compare across runs:\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir=/tmp/tfmodels/mnist_cnn_estimator\n",
    "```\n",
    "\n",
    "Or run the following (select Kernel --> Interrupt from the menu when you're done):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=$MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
